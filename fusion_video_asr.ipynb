{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# import cv2 as cv\n",
    "import numpy as np\n",
    "import glob\n",
    "# # from random import shuffle\n",
    "#from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow_hub as hub\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,Input, Flatten, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Bidirectional,LSTM,GlobalMaxPool1D,Dense,Concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import losses\n",
    "from keras.layers.merge import Multiply,Dot\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['frame', 'timestamp', 'confidence', 'success', 'pose_Tx', 'pose_Ty',\n",
       "       'pose_Tz', 'pose_Rx', 'pose_Ry', 'pose_Rz', 'gaze_0_x', 'gaze_0_y',\n",
       "       'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z', 'gaze_angle_x',\n",
       "       'gaze_angle_y', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r',\n",
       "       'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r',\n",
       "       'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r', 'AU01_c', 'AU02_c',\n",
       "       'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c', 'AU09_c', 'AU10_c', 'AU12_c',\n",
       "       'AU14_c', 'AU15_c', 'AU17_c', 'AU20_c', 'AU23_c', 'AU25_c', 'AU26_c',\n",
       "       'AU28_c', 'AU45_c'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('video_LLDs_FAUs/test_01_OpenFace2.1.0_Pose_gaze_AUs.csv')\n",
    "\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv')\n",
    "sample = pd.read_csv('video_LLDs_FAUs/training_001_OpenFace2.1.0_Pose_gaze_AUs.csv')\n",
    "sample_bow = pd.read_csv('video_BoVW/development_01_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv', header = None, delimiter = ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_cols = sample_bow.columns[2:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'video_BoVW/'\n",
    "bow_train = []\n",
    "c = 0\n",
    "for i in metadata['Participant_ID']:\n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    \n",
    "    if i.startswith('train') and file.startswith(i):\n",
    "      c+=1\n",
    "#       print(i)\n",
    "#       print(c)\n",
    "      \n",
    "#       print(file[0:4]=='trai'):\n",
    "        \n",
    "      data = pd.read_csv('video_BoVW/'+file, header = None, delimiter=';')\n",
    "      data = data[bow_cols]\n",
    "      bow_train.append(data)\n",
    "\n",
    "pose_cols = sample.columns[4:10]\n",
    "gaze_cols = sample.columns[10:18]\n",
    "au_cols = sample.columns[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'video_LLDs_FAUs/'\n",
    "pose_train = []\n",
    "gaze_train = []\n",
    "au_train = []\n",
    "c = 0\n",
    "for i in metadata['Participant_ID']:\n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    \n",
    "    if i.startswith('train') and file.startswith(i):\n",
    "      c+=1\n",
    "#       print(i)\n",
    "#       print(c)\n",
    "      \n",
    "#       print(file[0:4]=='trai'):\n",
    "        \n",
    "      feat = pd.read_csv('video_LLDs_FAUs/'+file)\n",
    "      feat = feat.iloc[::3, :]\n",
    "      pose_train.append(feat[pose_cols])\n",
    "      gaze_train.append(feat[gaze_cols])\n",
    "      au_train.append(feat[au_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'video_BoVW/'\n",
    "bow_dev = []\n",
    "c = 0\n",
    "for i in metadata['Participant_ID']:\n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    \n",
    "    if i.startswith('development') and file.startswith(i):\n",
    "      c+=1\n",
    "      #print(i)\n",
    "     # print(c)\n",
    "      \n",
    "#       print(file[0:4]=='trai'):\n",
    "        \n",
    "      data = pd.read_csv('video_BoVW/'+file, header = None, delimiter=';')\n",
    "      data = data[bow_cols]\n",
    "      bow_dev.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development_01 1\n",
      "development_02 2\n",
      "development_03 3\n",
      "development_04 4\n",
      "development_05 5\n",
      "development_06 6\n",
      "development_07 7\n",
      "development_08 8\n",
      "development_09 9\n",
      "development_10 10\n",
      "development_11 11\n",
      "development_12 12\n",
      "development_13 13\n",
      "development_14 14\n",
      "development_15 15\n",
      "development_16 16\n",
      "development_17 17\n",
      "development_18 18\n",
      "development_19 19\n",
      "development_20 20\n",
      "development_21 21\n",
      "development_22 22\n",
      "development_23 23\n",
      "development_24 24\n",
      "development_25 25\n",
      "development_26 26\n",
      "development_27 27\n",
      "development_28 28\n",
      "development_29 29\n",
      "development_30 30\n",
      "development_31 31\n",
      "development_32 32\n",
      "development_33 33\n",
      "development_34 34\n",
      "development_35 35\n",
      "development_36 36\n",
      "development_37 37\n",
      "development_38 38\n",
      "development_39 39\n",
      "development_40 40\n",
      "development_41 41\n",
      "development_42 42\n",
      "development_43 43\n",
      "development_44 44\n",
      "development_45 45\n",
      "development_46 46\n",
      "development_47 47\n",
      "development_48 48\n",
      "development_49 49\n",
      "development_50 50\n",
      "development_51 51\n",
      "development_52 52\n",
      "development_53 53\n",
      "development_54 54\n",
      "development_55 55\n",
      "development_56 56\n"
     ]
    }
   ],
   "source": [
    "path = 'video_LLDs_FAUs/'\n",
    "pose_dev = []\n",
    "gaze_dev = []\n",
    "au_dev = []\n",
    "c = 0\n",
    "for i in metadata['Participant_ID']:\n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    \n",
    "    if i.startswith('dev') and file.startswith(i):\n",
    "      c+=1\n",
    "      print(i,c)\n",
    "      \n",
    "#       print(file[0:4]=='trai'):\n",
    "        \n",
    "      feat = pd.read_csv('video_LLDs_FAUs/'+file)\n",
    "      feat = feat.iloc[::3, :]\n",
    "      pose_dev.append(feat[pose_cols])\n",
    "      gaze_dev.append(feat[gaze_cols])\n",
    "      au_dev.append(feat[au_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\",\"i am\",text)\n",
    "    text = re.sub(r\"i've\",\"i have\",text)\n",
    "    text = re.sub(r\"he's\",\"he is\",text)\n",
    "    text = re.sub(r\"she's\",\"she is\",text)\n",
    "    text = re.sub(r\"that's\",\"that is\",text)\n",
    "    text = re.sub(r\"that ' s\",\"that is\",text)\n",
    "    text = re.sub(r\"it's\",\"it is\",text)\n",
    "    text = re.sub(r\"that's\",\"that is\",text)\n",
    "    text = re.sub(r\"where's\",\"where is\",text)\n",
    "    text = re.sub(r\"what's\",\"what is\",text)\n",
    "    text = re.sub(r\"\\'ll\",\" will\",text) \n",
    "    text = re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text = re.sub(r\"\\'re\",\" are\",text)\n",
    "    text = re.sub(r\"\\'d\",\" would\",text)  \n",
    "    text = re.sub(r\"won't\", \"will not\",text)\n",
    "    text = re.sub(r\"don't\", \"do not\",text)\n",
    "    text = re.sub(r\"can't\",\"can not\", text)\n",
    "    text = re.sub(r\"hadn't\",\"had not\", text)\n",
    "    text = re.sub(r\"didn't\",\"did not\", text)\n",
    "    text = re.sub(r\"wouldn't\",\"would not\", text)\n",
    "    text = re.sub(r\"weren't\",\"were not\", text)\n",
    "    text = re.sub(r\"shouldn't\",\"should not\", text)\n",
    "    text = re.sub(r\"doesn't\",\"does not\", text)\n",
    "    text = re.sub(r\"couldn't\",\"could not\", text)\n",
    "    text = re.sub(r\"isn't\",\"is not\", text)\n",
    "    text = re.sub(r\"hasn't\",\"has not\", text)\n",
    "    text = re.sub(r\"wasn't\",\"was not\", text)\n",
    "    text = re.sub(r\"haven't\",\"have not\", text)\n",
    "    text = re.sub(r\"didn't\",\"did not\", text)\n",
    "    text = re.sub(r\"wouldnt'\",\"would not\", text)\n",
    "#     text = re.sub(r\"shouldnt'\",\"should not\", text)\n",
    "#     text = re.sub(r\"doesnt'\",\"does not\", text)\n",
    "#     text = re.sub(r\"couldnt'\",\"could not\", text)\n",
    "#     text = re.sub(r\"isnt'\",\"is not\", text)\n",
    "#     text = re.sub(r\"wasn't\",\"was not\", text)\n",
    "#     text = re.sub(r\"havent'\",\"have not\", text)\n",
    "    text = re.sub(r\"aren't\",\"are not\", text)\n",
    "#     text = re.sub(r\"werent\",\"were not\", text)\n",
    "    text = re.sub(r\" em \",\" them \", text)\n",
    "    text = re.sub(r\" there's \",\" there is \", text)\n",
    "    text = re.sub(r\"let's\",\"let us\", text)\n",
    "    text = re.sub(r\" who's \",\" who is \", text)\n",
    "    text = re.sub(r\"\\'s\",\"\", text)\n",
    "    text = re.sub(r\"'\",\"\", text)\n",
    "# #     text = re.sub(r\"arent\",\"are not\", text)\n",
    "#     text = re.sub(r\"  \",\" \", text)\n",
    "#     text = re.sub(r\"[-?!@#$%^&*(~,;)+=_`<>{}.\\\"']\",\"\",text)\n",
    "#     pattern = r'[^a-zA-z0-9\\s]'\n",
    "#     text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(data,cnt):\n",
    "#   Participant_transcript=[rows[0].split(\"\\t\") for rows in data if len(rows)>0 if rows[0].split(\"\\t\")[2] == \"Participant\" ]\n",
    "    if cnt==0:\n",
    "      Participant_transcript=[rows[0].split(\"\\t\") for rows in data if len(rows)>0 if rows[0].split(\"\\t\")[2] == \"Participant\" ]\n",
    "#   print(Participant_transcript)\n",
    "      words=[clean_text(x[3].strip()).strip() for x in Participant_transcript]\n",
    "    else:\n",
    "      words=[clean_text(x[2].strip()).strip() for x in data]\n",
    "      words.pop(0)\n",
    "#     print(words)\n",
    "    for ind,val in enumerate(words):\n",
    "        a=val.split(' ')\n",
    "        for ii,tp in enumerate(a):\n",
    "            if(len(tp)):\n",
    "                if(tp[0]=='<'and tp[-1]=='>'):\n",
    "                    a[ii]=tp[1:-1]\n",
    "                elif(tp[0]=='<' ):\n",
    "                    a[ii]=tp[1:]\n",
    "        words[ind]=' '.join(a).strip()        \n",
    "    m.append(len(words))\n",
    "#     print(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0713 00:24:12.409619 140520442545920 deprecation.py:323] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.VERSION\n",
    "sentences=[]\n",
    "target_asr=[]\n",
    "m=[]\n",
    "embed_fn = embed_useT('USE/')\n",
    "embedding_matrix=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR_transcripts/development_43_Transcript.csv\n",
      "(106, 512)\n",
      "ASR_transcripts/training_040_Transcript.csv\n",
      "(64, 512)\n",
      "ASR_transcripts/test_40_Transcript.csv\n",
      "ASR_transcripts/training_039_Transcript.csv\n",
      "(44, 512)\n",
      "ASR_transcripts/test_21_Transcript.csv\n",
      "ASR_transcripts/test_25_Transcript.csv\n",
      "ASR_transcripts/training_062_Transcript.csv\n",
      "(57, 512)\n",
      "ASR_transcripts/test_34_Transcript.csv\n",
      "ASR_transcripts/training_089_Transcript.csv\n",
      "(80, 512)\n",
      "ASR_transcripts/training_049_Transcript.csv\n",
      "(64, 512)\n",
      "ASR_transcripts/development_25_Transcript.csv\n",
      "(91, 512)\n",
      "ASR_transcripts/training_104_Transcript.csv\n",
      "(47, 512)\n",
      "ASR_transcripts/training_135_Transcript.csv\n",
      "(91, 512)\n",
      "ASR_transcripts/development_34_Transcript.csv\n",
      "(70, 512)\n",
      "ASR_transcripts/test_18_Transcript.csv\n",
      "ASR_transcripts/training_128_Transcript.csv\n",
      "(64, 512)\n",
      "ASR_transcripts/training_080_Transcript.csv\n",
      "(165, 512)\n",
      "ASR_transcripts/training_098_Transcript.csv\n",
      "(73, 512)\n",
      "ASR_transcripts/development_56_Transcript.csv\n",
      "(111, 512)\n",
      "ASR_transcripts/training_063_Transcript.csv\n",
      "(174, 512)\n",
      "ASR_transcripts/development_48_Transcript.csv\n",
      "(207, 512)\n",
      "ASR_transcripts/training_103_Transcript.csv\n",
      "(56, 512)\n",
      "ASR_transcripts/training_088_Transcript.csv\n",
      "(99, 512)\n",
      "ASR_transcripts/test_27_Transcript.csv\n",
      "ASR_transcripts/training_076_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/training_106_Transcript.csv\n",
      "(128, 512)\n",
      "ASR_transcripts/test_01_Transcript.csv\n",
      "ASR_transcripts/training_154_Transcript.csv\n",
      "(67, 512)\n",
      "ASR_transcripts/training_155_Transcript.csv\n",
      "(112, 512)\n",
      "ASR_transcripts/training_012_Transcript.csv\n",
      "(114, 512)\n",
      "ASR_transcripts/training_034_Transcript.csv\n",
      "(71, 512)\n",
      "ASR_transcripts/development_54_Transcript.csv\n",
      "(151, 512)\n",
      "ASR_transcripts/test_13_Transcript.csv\n",
      "ASR_transcripts/test_48_Transcript.csv\n",
      "ASR_transcripts/training_044_Transcript.csv\n",
      "(51, 512)\n",
      "ASR_transcripts/development_21_Transcript.csv\n",
      "(79, 512)\n",
      "ASR_transcripts/test_03_Transcript.csv\n",
      "ASR_transcripts/development_08_Transcript.csv\n",
      "(99, 512)\n",
      "ASR_transcripts/training_073_Transcript.csv\n",
      "(91, 512)\n",
      "ASR_transcripts/training_111_Transcript.csv\n",
      "(61, 512)\n",
      "ASR_transcripts/training_146_Transcript.csv\n",
      "(121, 512)\n",
      "ASR_transcripts/development_26_Transcript.csv\n",
      "(65, 512)\n",
      "ASR_transcripts/test_44_Transcript.csv\n",
      "ASR_transcripts/test_56_Transcript.csv\n",
      "ASR_transcripts/training_003_Transcript.csv\n",
      "(80, 512)\n",
      "ASR_transcripts/development_50_Transcript.csv\n",
      "(82, 512)\n",
      "ASR_transcripts/training_058_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/development_17_Transcript.csv\n",
      "(106, 512)\n",
      "ASR_transcripts/development_53_Transcript.csv\n",
      "(84, 512)\n",
      "ASR_transcripts/training_132_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/training_114_Transcript.csv\n",
      "(105, 512)\n",
      "ASR_transcripts/test_19_Transcript.csv\n",
      "ASR_transcripts/training_159_Transcript.csv\n",
      "(166, 512)\n",
      "ASR_transcripts/development_40_Transcript.csv\n",
      "(97, 512)\n",
      "ASR_transcripts/training_033_Transcript.csv\n",
      "(112, 512)\n",
      "ASR_transcripts/training_064_Transcript.csv\n",
      "(115, 512)\n",
      "ASR_transcripts/development_37_Transcript.csv\n",
      "(96, 512)\n",
      "ASR_transcripts/training_161_Transcript.csv\n",
      "(86, 512)\n",
      "ASR_transcripts/training_065_Transcript.csv\n",
      "(128, 512)\n",
      "ASR_transcripts/training_022_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/test_46_Transcript.csv\n",
      "ASR_transcripts/training_144_Transcript.csv\n",
      "(53, 512)\n",
      "ASR_transcripts/test_14_Transcript.csv\n",
      "ASR_transcripts/development_22_Transcript.csv\n",
      "(108, 512)\n",
      "ASR_transcripts/training_042_Transcript.csv\n",
      "(66, 512)\n",
      "ASR_transcripts/training_014_Transcript.csv\n",
      "(98, 512)\n",
      "ASR_transcripts/development_52_Transcript.csv\n",
      "(121, 512)\n",
      "ASR_transcripts/development_30_Transcript.csv\n",
      "(79, 512)\n",
      "ASR_transcripts/test_35_Transcript.csv\n",
      "ASR_transcripts/training_057_Transcript.csv\n",
      "(189, 512)\n",
      "ASR_transcripts/training_060_Transcript.csv\n",
      "(122, 512)\n",
      "ASR_transcripts/training_095_Transcript.csv\n",
      "(58, 512)\n",
      "ASR_transcripts/training_029_Transcript.csv\n",
      "(151, 512)\n",
      "ASR_transcripts/test_16_Transcript.csv\n",
      "ASR_transcripts/training_145_Transcript.csv\n",
      "(89, 512)\n",
      "ASR_transcripts/training_110_Transcript.csv\n",
      "(75, 512)\n",
      "ASR_transcripts/training_122_Transcript.csv\n",
      "(66, 512)\n",
      "ASR_transcripts/training_113_Transcript.csv\n",
      "(130, 512)\n",
      "ASR_transcripts/training_102_Transcript.csv\n",
      "(112, 512)\n",
      "ASR_transcripts/training_108_Transcript.csv\n",
      "(75, 512)\n",
      "ASR_transcripts/training_130_Transcript.csv\n",
      "(80, 512)\n",
      "ASR_transcripts/training_125_Transcript.csv\n",
      "(134, 512)\n",
      "ASR_transcripts/training_099_Transcript.csv\n",
      "(105, 512)\n",
      "ASR_transcripts/test_12_Transcript.csv\n",
      "ASR_transcripts/development_05_Transcript.csv\n",
      "(116, 512)\n",
      "ASR_transcripts/development_11_Transcript.csv\n",
      "(111, 512)\n",
      "ASR_transcripts/test_28_Transcript.csv\n",
      "ASR_transcripts/development_31_Transcript.csv\n",
      "(114, 512)\n",
      "ASR_transcripts/training_032_Transcript.csv\n",
      "(59, 512)\n",
      "ASR_transcripts/training_139_Transcript.csv\n",
      "(56, 512)\n",
      "ASR_transcripts/training_009_Transcript.csv\n",
      "(88, 512)\n",
      "ASR_transcripts/training_147_Transcript.csv\n",
      "(78, 512)\n",
      "ASR_transcripts/training_052_Transcript.csv\n",
      "(119, 512)\n",
      "ASR_transcripts/training_015_Transcript.csv\n",
      "(48, 512)\n",
      "ASR_transcripts/development_55_Transcript.csv\n",
      "(141, 512)\n",
      "ASR_transcripts/test_10_Transcript.csv\n",
      "ASR_transcripts/development_47_Transcript.csv\n",
      "(94, 512)\n",
      "ASR_transcripts/development_36_Transcript.csv\n",
      "(115, 512)\n",
      "ASR_transcripts/training_140_Transcript.csv\n",
      "(87, 512)\n",
      "ASR_transcripts/development_49_Transcript.csv\n",
      "(75, 512)\n",
      "ASR_transcripts/training_151_Transcript.csv\n",
      "(63, 512)\n",
      "ASR_transcripts/test_24_Transcript.csv\n",
      "ASR_transcripts/training_023_Transcript.csv\n",
      "(67, 512)\n",
      "ASR_transcripts/training_091_Transcript.csv\n",
      "(60, 512)\n",
      "ASR_transcripts/training_020_Transcript.csv\n",
      "(53, 512)\n",
      "ASR_transcripts/test_54_Transcript.csv\n",
      "ASR_transcripts/training_152_Transcript.csv\n",
      "(116, 512)\n",
      "ASR_transcripts/training_072_Transcript.csv\n",
      "(72, 512)\n",
      "ASR_transcripts/training_071_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/test_37_Transcript.csv\n",
      "ASR_transcripts/test_20_Transcript.csv\n",
      "ASR_transcripts/training_090_Transcript.csv\n",
      "(114, 512)\n",
      "ASR_transcripts/development_16_Transcript.csv\n",
      "(68, 512)\n",
      "ASR_transcripts/training_068_Transcript.csv\n",
      "(92, 512)\n",
      "ASR_transcripts/test_22_Transcript.csv\n",
      "ASR_transcripts/training_153_Transcript.csv\n",
      "(97, 512)\n",
      "ASR_transcripts/training_124_Transcript.csv\n",
      "(128, 512)\n",
      "ASR_transcripts/training_123_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_085_Transcript.csv\n",
      "(90, 512)\n",
      "ASR_transcripts/training_018_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_017_Transcript.csv\n",
      "(108, 512)\n",
      "ASR_transcripts/test_29_Transcript.csv\n",
      "ASR_transcripts/development_02_Transcript.csv\n",
      "(72, 512)\n",
      "ASR_transcripts/training_024_Transcript.csv\n",
      "(55, 512)\n",
      "ASR_transcripts/test_04_Transcript.csv\n",
      "ASR_transcripts/development_44_Transcript.csv\n",
      "(99, 512)\n",
      "ASR_transcripts/training_007_Transcript.csv\n",
      "(62, 512)\n",
      "ASR_transcripts/development_38_Transcript.csv\n",
      "(62, 512)\n",
      "ASR_transcripts/test_26_Transcript.csv\n",
      "ASR_transcripts/test_15_Transcript.csv\n",
      "ASR_transcripts/test_51_Transcript.csv\n",
      "ASR_transcripts/training_107_Transcript.csv\n",
      "(88, 512)\n",
      "ASR_transcripts/training_082_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_031_Transcript.csv\n",
      "(83, 512)\n",
      "ASR_transcripts/training_045_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_067_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/training_131_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/test_50_Transcript.csv\n",
      "ASR_transcripts/training_156_Transcript.csv\n",
      "(170, 512)\n",
      "ASR_transcripts/training_019_Transcript.csv\n",
      "(92, 512)\n",
      "ASR_transcripts/development_35_Transcript.csv\n",
      "(89, 512)\n",
      "ASR_transcripts/training_126_Transcript.csv\n",
      "(99, 512)\n",
      "ASR_transcripts/training_143_Transcript.csv\n",
      "(80, 512)\n",
      "ASR_transcripts/test_43_Transcript.csv\n",
      "ASR_transcripts/development_13_Transcript.csv\n",
      "(83, 512)\n",
      "ASR_transcripts/test_06_Transcript.csv\n",
      "ASR_transcripts/test_07_Transcript.csv\n",
      "ASR_transcripts/training_150_Transcript.csv\n",
      "(79, 512)\n",
      "ASR_transcripts/development_01_Transcript.csv\n",
      "(77, 512)\n",
      "ASR_transcripts/development_24_Transcript.csv\n",
      "(78, 512)\n",
      "ASR_transcripts/test_33_Transcript.csv\n",
      "ASR_transcripts/training_105_Transcript.csv\n",
      "(63, 512)\n",
      "ASR_transcripts/training_163_Transcript.csv\n",
      "(97, 512)\n",
      "ASR_transcripts/training_001_Transcript.csv\n",
      "(99, 512)\n",
      "ASR_transcripts/development_33_Transcript.csv\n",
      "(106, 512)\n",
      "ASR_transcripts/training_142_Transcript.csv\n",
      "(91, 512)\n",
      "ASR_transcripts/test_02_Transcript.csv\n",
      "ASR_transcripts/development_51_Transcript.csv\n",
      "(101, 512)\n",
      "ASR_transcripts/training_083_Transcript.csv\n",
      "(93, 512)\n",
      "ASR_transcripts/training_011_Transcript.csv\n",
      "(78, 512)\n",
      "ASR_transcripts/training_056_Transcript.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 512)\n",
      "ASR_transcripts/training_053_Transcript.csv\n",
      "(131, 512)\n",
      "ASR_transcripts/development_15_Transcript.csv\n",
      "(117, 512)\n",
      "ASR_transcripts/training_078_Transcript.csv\n",
      "(65, 512)\n",
      "ASR_transcripts/test_45_Transcript.csv\n",
      "ASR_transcripts/test_11_Transcript.csv\n",
      "ASR_transcripts/development_19_Transcript.csv\n",
      "(107, 512)\n",
      "ASR_transcripts/training_116_Transcript.csv\n",
      "(71, 512)\n",
      "ASR_transcripts/training_079_Transcript.csv\n",
      "(164, 512)\n",
      "ASR_transcripts/development_07_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/development_41_Transcript.csv\n",
      "(60, 512)\n",
      "ASR_transcripts/training_004_Transcript.csv\n",
      "(203, 512)\n",
      "ASR_transcripts/development_29_Transcript.csv\n",
      "(82, 512)\n",
      "ASR_transcripts/development_18_Transcript.csv\n",
      "(141, 512)\n",
      "ASR_transcripts/training_120_Transcript.csv\n",
      "(127, 512)\n",
      "ASR_transcripts/test_09_Transcript.csv\n",
      "ASR_transcripts/training_086_Transcript.csv\n",
      "(92, 512)\n",
      "ASR_transcripts/training_074_Transcript.csv\n",
      "(75, 512)\n",
      "ASR_transcripts/training_093_Transcript.csv\n",
      "(90, 512)\n",
      "ASR_transcripts/training_092_Transcript.csv\n",
      "(123, 512)\n",
      "ASR_transcripts/development_46_Transcript.csv\n",
      "(74, 512)\n",
      "ASR_transcripts/training_084_Transcript.csv\n",
      "(120, 512)\n",
      "ASR_transcripts/training_021_Transcript.csv\n",
      "(78, 512)\n",
      "ASR_transcripts/test_17_Transcript.csv\n",
      "ASR_transcripts/training_013_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/training_148_Transcript.csv\n",
      "(123, 512)\n",
      "ASR_transcripts/test_55_Transcript.csv\n",
      "ASR_transcripts/development_32_Transcript.csv\n",
      "(71, 512)\n",
      "ASR_transcripts/development_27_Transcript.csv\n",
      "(112, 512)\n",
      "ASR_transcripts/development_23_Transcript.csv\n",
      "(104, 512)\n",
      "ASR_transcripts/training_081_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/training_087_Transcript.csv\n",
      "(111, 512)\n",
      "ASR_transcripts/training_097_Transcript.csv\n",
      "(98, 512)\n",
      "ASR_transcripts/test_53_Transcript.csv\n",
      "ASR_transcripts/training_047_Transcript.csv\n",
      "(44, 512)\n",
      "ASR_transcripts/training_050_Transcript.csv\n",
      "(86, 512)\n",
      "ASR_transcripts/test_36_Transcript.csv\n",
      "ASR_transcripts/training_133_Transcript.csv\n",
      "(71, 512)\n",
      "ASR_transcripts/training_157_Transcript.csv\n",
      "(122, 512)\n",
      "ASR_transcripts/training_158_Transcript.csv\n",
      "(57, 512)\n",
      "ASR_transcripts/training_117_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_061_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/test_23_Transcript.csv\n",
      "ASR_transcripts/training_134_Transcript.csv\n",
      "(114, 512)\n",
      "ASR_transcripts/training_026_Transcript.csv\n",
      "(111, 512)\n",
      "ASR_transcripts/training_035_Transcript.csv\n",
      "(118, 512)\n",
      "ASR_transcripts/test_41_Transcript.csv\n",
      "ASR_transcripts/training_005_Transcript.csv\n",
      "(100, 512)\n",
      "ASR_transcripts/training_118_Transcript.csv\n",
      "(68, 512)\n",
      "ASR_transcripts/training_054_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/test_30_Transcript.csv\n",
      "ASR_transcripts/training_046_Transcript.csv\n",
      "(100, 512)\n",
      "ASR_transcripts/training_055_Transcript.csv\n",
      "(61, 512)\n",
      "ASR_transcripts/training_037_Transcript.csv\n",
      "(147, 512)\n",
      "ASR_transcripts/training_119_Transcript.csv\n",
      "(118, 512)\n",
      "ASR_transcripts/development_12_Transcript.csv\n",
      "(59, 512)\n",
      "ASR_transcripts/development_20_Transcript.csv\n",
      "(98, 512)\n",
      "ASR_transcripts/training_043_Transcript.csv\n",
      "(97, 512)\n",
      "ASR_transcripts/test_39_Transcript.csv\n",
      "ASR_transcripts/development_04_Transcript.csv\n",
      "(78, 512)\n",
      "ASR_transcripts/training_059_Transcript.csv\n",
      "(91, 512)\n",
      "ASR_transcripts/test_05_Transcript.csv\n",
      "ASR_transcripts/development_03_Transcript.csv\n",
      "(72, 512)\n",
      "ASR_transcripts/training_160_Transcript.csv\n",
      "(116, 512)\n",
      "ASR_transcripts/development_28_Transcript.csv\n",
      "(115, 512)\n",
      "ASR_transcripts/training_112_Transcript.csv\n",
      "(54, 512)\n",
      "ASR_transcripts/training_109_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/training_100_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/training_075_Transcript.csv\n",
      "(122, 512)\n",
      "ASR_transcripts/training_096_Transcript.csv\n",
      "(113, 512)\n",
      "ASR_transcripts/training_038_Transcript.csv\n",
      "(70, 512)\n",
      "ASR_transcripts/test_32_Transcript.csv\n",
      "ASR_transcripts/training_025_Transcript.csv\n",
      "(86, 512)\n",
      "ASR_transcripts/test_47_Transcript.csv\n",
      "ASR_transcripts/development_14_Transcript.csv\n",
      "(203, 512)\n",
      "ASR_transcripts/training_041_Transcript.csv\n",
      "(75, 512)\n",
      "ASR_transcripts/training_006_Transcript.csv\n",
      "(149, 512)\n",
      "ASR_transcripts/test_31_Transcript.csv\n",
      "ASR_transcripts/test_49_Transcript.csv\n",
      "ASR_transcripts/test_42_Transcript.csv\n",
      "ASR_transcripts/training_138_Transcript.csv\n",
      "(82, 512)\n",
      "ASR_transcripts/training_077_Transcript.csv\n",
      "(92, 512)\n",
      "ASR_transcripts/training_070_Transcript.csv\n",
      "(153, 512)\n",
      "ASR_transcripts/training_048_Transcript.csv\n",
      "(42, 512)\n",
      "ASR_transcripts/development_06_Transcript.csv\n",
      "(81, 512)\n",
      "ASR_transcripts/test_38_Transcript.csv\n",
      "ASR_transcripts/training_036_Transcript.csv\n",
      "(85, 512)\n",
      "ASR_transcripts/training_101_Transcript.csv\n",
      "(109, 512)\n",
      "ASR_transcripts/test_08_Transcript.csv\n",
      "ASR_transcripts/training_030_Transcript.csv\n",
      "(54, 512)\n",
      "ASR_transcripts/training_002_Transcript.csv\n",
      "(94, 512)\n",
      "ASR_transcripts/training_010_Transcript.csv\n",
      "(88, 512)\n",
      "ASR_transcripts/training_162_Transcript.csv\n",
      "(83, 512)\n",
      "ASR_transcripts/training_137_Transcript.csv\n",
      "(64, 512)\n",
      "ASR_transcripts/development_45_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/training_121_Transcript.csv\n",
      "(66, 512)\n",
      "ASR_transcripts/development_10_Transcript.csv\n",
      "(107, 512)\n",
      "ASR_transcripts/test_52_Transcript.csv\n",
      "ASR_transcripts/development_42_Transcript.csv\n",
      "(59, 512)\n",
      "ASR_transcripts/training_069_Transcript.csv\n",
      "(118, 512)\n",
      "ASR_transcripts/training_127_Transcript.csv\n",
      "(72, 512)\n",
      "ASR_transcripts/training_051_Transcript.csv\n",
      "(145, 512)\n",
      "ASR_transcripts/training_136_Transcript.csv\n",
      "(183, 512)\n",
      "ASR_transcripts/training_027_Transcript.csv\n",
      "(103, 512)\n",
      "ASR_transcripts/training_141_Transcript.csv\n",
      "(67, 512)\n",
      "ASR_transcripts/training_008_Transcript.csv\n",
      "(76, 512)\n",
      "ASR_transcripts/training_115_Transcript.csv\n",
      "(100, 512)\n",
      "ASR_transcripts/training_129_Transcript.csv\n",
      "(86, 512)\n",
      "ASR_transcripts/training_149_Transcript.csv\n",
      "(118, 512)\n",
      "ASR_transcripts/training_066_Transcript.csv\n",
      "(69, 512)\n",
      "ASR_transcripts/development_39_Transcript.csv\n",
      "(95, 512)\n",
      "ASR_transcripts/training_016_Transcript.csv\n",
      "(82, 512)\n",
      "ASR_transcripts/development_09_Transcript.csv\n",
      "(107, 512)\n",
      "ASR_transcripts/training_028_Transcript.csv\n",
      "(84, 512)\n",
      "ASR_transcripts/training_094_Transcript.csv\n",
      "(163, 512)\n",
      "ASR_transcripts/metadata.csv\n",
      "(219, 512)\n"
     ]
    }
   ],
   "source": [
    "emb_dim_asr=400\n",
    "pth=[\"ASR_transcripts/*.csv\"]\n",
    "for cnt,p in enumerate(pth):\n",
    "  cnt+=1\n",
    "  if cnt>0:\n",
    "    for filename in glob.glob(p):#CHANGE PATH\n",
    "        print(filename)\n",
    "        if 'test' not in filename:\n",
    "          if(cnt==0):\n",
    "            target_asr.append(filename.split(\"/\")[-1].split(\"_\")[0])\n",
    "          else:\n",
    "            target_asr.append(filename.split(\"/\")[-1].split(\".\")[0][:-11])\n",
    "          with open(filename,\"rt\") as f:\n",
    "            data=csv.reader(f)\n",
    "            msg_txt=sent(data,cnt)\n",
    "      #       print(msg_txt)  \n",
    "            emb=embed_fn(msg_txt)\n",
    "            a = np.zeros((emb_dim_asr-emb.shape[0], 512))\n",
    "            x=np.vstack((a,emb))\n",
    "            embedding_matrix.append(x)\n",
    "            print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 400, 512)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_asr=[]\n",
    "pth=[\"metadata.csv\"]\n",
    "for p in pth:\n",
    "    with open(p,\"rt\") as f:\n",
    "        l_data=csv.reader(f)\n",
    "        for r in l_data:\n",
    "          if len(r):\n",
    "            if r[0] in target_asr and ('training' in r[0]):\n",
    "              train_data_asr.append(embedding_matrix[target_asr.index(r[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_asr=np.asarray(train_data_asr,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_asr=[]\n",
    "pth=[\"metadata.csv\"]\n",
    "for p in pth:\n",
    "    with open(p,\"rt\") as f:\n",
    "        l_data=csv.reader(f)\n",
    "        for r in l_data:\n",
    "          if len(r):\n",
    "            if r[0] in target_asr and ('development' in r[0]):\n",
    "              val_data_asr.append(embedding_matrix[target_asr.index(r[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_asr=np.asarray(val_data_asr,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163, 400, 512)\n",
      "(56, 400, 512)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_asr.shape)\n",
    "print(val_data_asr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/my_env/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/siddharth/my_env/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/siddharth/my_env/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "\n",
    "for i,v in metadata.iterrows():\n",
    "  if v['Participant_ID'].startswith('train'):\n",
    "    target.append(v['PHQ_Score'])\n",
    "\n",
    "dev_target = []\n",
    "\n",
    "for i,v in metadata.iterrows():\n",
    "  if v['Participant_ID'].startswith('dev'):\n",
    "    dev_target.append(v['PHQ_Score'])\n",
    "\n",
    "for i in pose_train:\n",
    "#   print(i)\n",
    "  i['pose_Tx'] = i['pose_Tx']/100\n",
    "  i['pose_Ty'] = i['pose_Ty']/100\n",
    "  i['pose_Tz'] = i['pose_Tz']/100\n",
    "\n",
    "# c = 0\n",
    "# for i in pose_test:\n",
    "#   print(c, end = ' ')\n",
    "#   c+=1\n",
    "#   print(i)\n",
    "  i['pose_Tx'] = i['pose_Tx']/100\n",
    "  i['pose_Ty'] = i['pose_Ty']/100\n",
    "  i['pose_Tz'] = i['pose_Tz']/100\n",
    "\n",
    "c = 0\n",
    "for i in pose_dev:\n",
    "  print(c,end = ' ')\n",
    "  c+=1\n",
    "#   print(i)\n",
    "  i['pose_Tx'] = i['pose_Tx']/100\n",
    "  i['pose_Ty'] = i['pose_Ty']/100\n",
    "  i['pose_Tz'] = i['pose_Tz']/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 15000\n",
    "\n",
    "train_seq_pose = pad_sequences(pose_train, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "# test_seq_pose = pad_sequences(pose_test, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "dev_seq_pose = pad_sequences(pose_dev, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "\n",
    "train_seq_gaze = pad_sequences(gaze_train, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "# test_seq_gaze = pad_sequences(gaze_test, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "dev_seq_gaze =  pad_sequences(gaze_dev, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "\n",
    "train_seq_au = pad_sequences(au_train, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "# test_seq_au = pad_sequences(au_test, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "dev_seq_au = pad_sequences(au_dev, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "\n",
    "train_seq_bow = pad_sequences(bow_train, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "# test_seq_bow = pad_sequences(bow_test, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n",
    "dev_seq_bow = pad_sequences(bow_dev, maxlen=MAX_SEQ_LEN,dtype = 'float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.reshape(target,[-1,1])\n",
    "dev_target = np.reshape(dev_target,[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feedf_network(xx,dp):\n",
    "#   with tf.variable_scope('FUSION'):\n",
    "#     logits=tf.layers.batch_normalization(tf.nn.relu(tf.layers.dense(xx,500)))\n",
    "#     logits=tf.nn.dropout(logits,keep_prob=dp)\n",
    "#     logits=tf.layers.batch_normalization(tf.nn.relu(tf.layers.dense(logits,100)))\n",
    "#     logits=tf.nn.dropout(logits,keep_prob=dp)\n",
    "#     logits=tf.layers.batch_normalization(tf.nn.relu(tf.layers.dense(logits,60)))\n",
    "#     logits= tf.layers.dense(logits,1)\n",
    "    \n",
    "#     return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# print(\"network started\")\n",
    "# true_targets = tf.placeholder(shape=(None,1),dtype = tf.float32)\n",
    "\n",
    "# input_bow = tf.placeholder(shape=(None,MAX_SEQ_LEN,100),dtype = tf.float32)\n",
    "# context_layer_bow = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = False))(input_bow)\n",
    "# print('bow',context_layer_bow.shape)\n",
    "# input_pose = tf.placeholder(shape=(None,MAX_SEQ_LEN,6),dtype = tf.float32)\n",
    "# context_layer_pose = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = False))(input_pose)\n",
    "# print('pose',context_layer_pose.shape)\n",
    "\n",
    "# input_gaze = tf.placeholder(shape=(None,MAX_SEQ_LEN,8),dtype = tf.float32, name='GAZE')\n",
    "# context_layer_gaze = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = False))(input_gaze)\n",
    "# print('gaze',context_layer_gaze.shape)\n",
    "\n",
    "# input_au = tf.placeholder(shape=(None,MAX_SEQ_LEN,35),dtype = tf.float32)\n",
    "# context_layer_au = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = False))(input_au)\n",
    "# print('au',context_layer_au.shape)\n",
    "# ############################################################################# asr\n",
    "\n",
    "# X_asr=tf.placeholder(tf.float32,[None,emb_dim_asr,512])\n",
    "# mx_asr=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=True))(X_asr)\n",
    "# print('mx',mx_asr.shape)\n",
    "# context_X_asr=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=True))(mx_asr)\n",
    "# print('context_asr',context_X_asr.shape)\n",
    "# context_X_asr=tf.reduce_sum(context_X_asr,1)\n",
    "# print('asr',context_X_asr.shape)\n",
    "#  ##############################################################################################\n",
    "# fusion_inp=tf.concat([context_X_asr,context_layer_bow,context_layer_pose,context_layer_gaze,\n",
    "#                       context_layer_au], 1)\n",
    "# print('fusion',fusion_inp.shape)\n",
    "\n",
    "\n",
    "# fused_logits=feedf_network(fusion_inp,1.0)\n",
    "\n",
    "# loss=tf.reduce_mean(tf.losses.mean_squared_error(labels=true_targets,predictions=fused_logits))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(loss)\n",
    "# print('graph ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 00:26:01.131720 140520442545920 deprecation.py:506] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0713 00:26:01.136654 140520442545920 deprecation.py:506] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0713 00:26:01.137952 140520442545920 deprecation.py:506] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0713 00:26:01.138875 140520442545920 deprecation.py:506] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network started\n",
      "fusion done\n",
      "Graph ready\n"
     ]
    }
   ],
   "source": [
    "#NETWORK DEFINITION\n",
    "tf.reset_default_graph()\n",
    "print(\"network started\")\n",
    "true_targets = tf.placeholder(shape=(None,1),dtype = tf.float32)\n",
    "\n",
    "input_bow = tf.placeholder(shape=(None,MAX_SEQ_LEN,100),dtype = tf.float32)\n",
    "context_layer_bow = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = True))(input_bow)\n",
    "\n",
    "input_pose = tf.placeholder(shape=(None,MAX_SEQ_LEN,6),dtype = tf.float32)\n",
    "context_layer_pose = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = True))(input_pose)\n",
    "\n",
    "\n",
    "input_gaze = tf.placeholder(shape=(None,MAX_SEQ_LEN,8),dtype = tf.float32, name='GAZE')\n",
    "context_layer_gaze = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = True))(input_gaze)\n",
    "\n",
    "\n",
    "input_au = tf.placeholder(shape=(None,MAX_SEQ_LEN,35),dtype = tf.float32)\n",
    "context_layer_au = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = True))(input_au)\n",
    "\n",
    "\n",
    "stacked_layer_visual = tf.stack([context_layer_bow,context_layer_pose, context_layer_gaze,context_layer_au], \n",
    "                                axis = -1)\n",
    "\n",
    "concat_layer_visual = tf.keras.layers.Concatenate()([context_layer_bow, context_layer_pose, context_layer_gaze,\n",
    "                                                     context_layer_au])\n",
    "\n",
    "atten_dense_visual = tf.keras.layers.Dense(100)(concat_layer_visual)\n",
    "atten_visual_out = tf.keras.layers.Dense(4,activation='softmax')(atten_dense_visual)\n",
    "\n",
    "atten_visual_out_re = tf.reshape(atten_visual_out,[tf.shape(atten_visual_out)[0],tf.shape(atten_visual_out)[1],1,-1])\n",
    "\n",
    "visual_out_fin = tf.multiply(atten_visual_out_re,stacked_layer_visual)\n",
    "\n",
    "visual_out_fin = tf.reshape(visual_out_fin,[tf.shape(visual_out_fin)[0],tf.shape(visual_out_fin)[1],400])\n",
    "\n",
    "fused_visual_context_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences = True))(visual_out_fin)\n",
    "\n",
    "visual_fused_pool = tf.keras.layers.GlobalMaxPool1D()(fused_visual_context_layer)\n",
    "fc_visual=tf.keras.layers.Dense(128,activation='relu')(visual_fused_pool)\n",
    "\n",
    "# print(\"video completed\")\n",
    "# ################################################################## audio\n",
    "\n",
    "# X_mfcc=tf.placeholder(tf.float32,[None,emb_dim_mfcc,78])\n",
    "# # mx=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=False))(X_mfcc)\n",
    "# context_X_mfcc=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=False))(X_mfcc)\n",
    "# fc_X_mfcc=tf.keras.layers.Dense(100,activation='relu')(context_X_mfcc)\n",
    "\n",
    "\n",
    "# X_ege=tf.placeholder(tf.float32,[None,emb_dim_ege,88])\n",
    "# # mx_ege=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=False))(X_ege)\n",
    "# context_X_ege=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=False))(X_ege)\n",
    "# fc_X_ege=tf.keras.layers.Dense(100,activation='relu')(context_X_ege)\n",
    "\n",
    "\n",
    "# X_BOAW_ege=tf.placeholder(tf.float32,[None,emb_dim_BOAW_ege,100])\n",
    "# # mx_BOAW_ege=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=False))(X_BOAW_ege)\n",
    "# context_X_BOAW_ege=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=False))(X_BOAW_ege)\n",
    "# fc_X_BOAW_ege=tf.keras.layers.Dense(100,activation='relu')(context_X_BOAW_ege)\n",
    "\n",
    "# X_BOAW_mfcc=tf.placeholder(tf.float32,[None,emb_dim_BOAW_mfcc,100])\n",
    "# # mx_BOAW_mfcc=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=False))(X_BOAW_mfcc)\n",
    "# context_X_BOAW_mfcc=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=False))(X_BOAW_mfcc)\n",
    "# fc_X_BOAW_mfcc=tf.keras.layers.Dense(100,activation='relu')(context_X_BOAW_mfcc)\n",
    "\n",
    "# # stacked_layer_audio=tf.stack([mx,mx_ege,mx_BOAW_mfcc,mx_BOAW_ege],axis=-1)#bs,100,5\n",
    "# concat_layer_audio=tf.keras.layers.Concatenate()([context_X_mfcc,context_X_ege, context_X_BOAW_mfcc,context_X_BOAW_ege])\n",
    "# stacked_layer_audio=tf.stack([context_X_mfcc,context_X_ege, context_X_BOAW_mfcc,context_X_BOAW_ege],axis=-1)#bs,100,5\n",
    "# # # concat_layer_audio=tf.keras.layers.Concatenate()([context_X_mfcc,context_X_BOAW_mfcc])\n",
    "\n",
    "# atten_audio_dense=tf.keras.layers.Dense(128,activation='relu')(concat_layer_audio)\n",
    "# atten_audio_out=tf.keras.layers.Dense(4,activation='softmax')(atten_audio_dense)#bs,5\n",
    "\n",
    "# out_audio_fin = tf.multiply(tf.expand_dims(atten_audio_out,1),stacked_layer_audio)\n",
    "# out_audio_fin1 = tf.reshape(out_audio_fin,[tf.shape(out_audio_fin)[0],out_audio_fin.shape[1]*out_audio_fin.shape[2]])\n",
    "# fc_audio=tf.keras.layers.Dense(128,activation='relu')(out_audio_fin1)\n",
    "\n",
    "# print(\"audio done\")\n",
    "# ############################################################################# asr\n",
    "\n",
    "X_asr=tf.placeholder(tf.float32,[None,emb_dim_asr,512])\n",
    "mx_asr=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=True))(X_asr)\n",
    "context_X_asr=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200,return_sequences=True))(mx_asr)\n",
    "context_X_asr=tf.reduce_sum(context_X_asr,1)\n",
    "fc_X_asr=tf.keras.layers.Dense(128,activation='relu')(context_X_asr)\n",
    "# print(\"transcript done\")\n",
    "#####################################################FUSION LAYER\n",
    "\n",
    "fused_stacked_layer=tf.stack([fc_visual,fc_X_asr],axis=1)#bs,128,5\n",
    "fused_concat_layer=tf.keras.layers.Concatenate()([fc_visual,fc_X_asr])\n",
    "\n",
    "fused_atten_dense=tf.keras.layers.Dense(128,activation='relu')(fused_concat_layer)\n",
    "fused_atten_out=tf.keras.layers.Dense(2,activation='softmax')(fused_atten_dense)\n",
    "\n",
    "fused_out_fin = tf.multiply(tf.expand_dims(fused_atten_out,2),fused_stacked_layer)\n",
    "\n",
    "fused_out_fin = tf.reshape(fused_out_fin,[tf.shape(fused_out_fin)[0],fused_out_fin.shape[1]*fused_out_fin.shape[2]])\n",
    "fused_fc=tf.keras.layers.Dense(128,activation='relu')(fused_out_fin)\n",
    "fused_fc=tf.keras.layers.Dense(64,activation='relu')(fused_fc)\n",
    "fused_logits=tf.keras.layers.Dense(1)(fused_fc)\n",
    " ##############################################################################################\n",
    "print(\"fusion done\")\n",
    "\n",
    "loss=tf.reduce_mean(tf.losses.mean_squared_error(labels=true_targets,predictions=fused_logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)\n",
    "print('Graph ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "path = \"fused_model3/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 00:26:13.196659 140520442545920 deprecation.py:323] From /home/siddharth/my_env/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss Currently :  23.875874\n",
      "Epoch: 0\n",
      "Starting..  7.9192114 ==11.8700285 ==26.491695 ==45.8636 ==36.616737 ==9.855001 ==8.951131 ==3.7976074 ==7.8412085 ==12.130108 ==14.887912 ==5.491971 ==20.224636 ==10.195732 ==25.945179 ==24.663723 ===\n",
      "Loss in epoch:0 = 17.046592712402344\n",
      "Dev Loss in epoch:0 = 24.766494750976562\n",
      "Epoch: 1\n",
      "Starting..  6.8696227 ==11.472746 ==25.27811 ==42.398033 ==34.637135 ==10.375821 ==8.226828 ==3.8846676 ==7.2276163 ==12.506052 ==14.931753 ==5.6001277 ==19.00094 ==10.497817 ==26.177975 ==23.92963 ===\n",
      "Loss in epoch:1 = 16.438429668545723\n",
      "Dev Loss in epoch:1 = 24.94010353088379\n",
      "Epoch: 2\n",
      "Starting..  5.38994 ==11.51731 ==24.526684 ==35.62223 ==31.623001 ==10.50532 ==10.812901 ==4.720414 ==7.4261465 ==11.22206 ==12.834195 ==4.161871 ==14.826553 ==7.967057 ==25.0181 ==22.090633 ===\n",
      "Loss in epoch:2 = 15.016526073217392\n",
      "Dev Loss in epoch:2 = 25.19040870666504\n",
      "Epoch: 3\n",
      "Starting..  5.2640624 ==9.310728 ==23.540466 ==33.721714 ==29.84994 ==7.735513 ==9.947987 ==5.209798 ==6.8526773 ==11.110423 ==12.177722 ==4.0778666 ==15.958174 ==8.229951 ==24.741236 ==24.536137 ===\n",
      "Loss in epoch:3 = 14.516524612903595\n",
      "Dev Loss in epoch:3 = 28.350107192993164\n",
      "Epoch: 4\n",
      "Starting..  6.3056135 ==10.542174 ==23.572147 ==29.781565 ==27.354553 ==10.226463 ==12.623845 ==6.0311155 ==6.775744 ==11.213737 ==12.082321 ==3.8174312 ==13.365606 ==6.552128 ==24.21037 ==23.837322 ===\n",
      "Loss in epoch:4 = 14.268258467316628\n",
      "Dev Loss in epoch:4 = 29.412851333618164\n",
      "Epoch: 5\n",
      "Starting..  7.4555006 ==10.93368 ==24.075008 ==27.366146 ==25.848557 ==6.095743 ==12.704407 ==7.391318 ==7.527124 ==13.253596 ==12.4090185 ==3.8704517 ==12.272142 ==4.634896 ==23.971369 ==25.813223 ===\n",
      "Loss in epoch:5 = 14.101386204361916\n",
      "Dev Loss in epoch:5 = 34.259002685546875\n",
      "Epoch: 6\n",
      "Starting..  10.545725 ==11.5952835 ==26.32486 ==30.043116 ==25.498402 ==4.493521 ==11.768939 ==11.177162 ==9.998541 ==17.641973 ==14.889292 ==3.8974872 ==14.54007 ==6.1458387 ==23.876226 ==26.866308 ===\n",
      "Loss in epoch:6 = 15.581421494483948\n",
      "Dev Loss in epoch:6 = 35.902462005615234\n",
      "Epoch: 7\n",
      "Starting..  15.916433 ==13.560364 ==30.192242 ==46.784462 ==32.466198 ==10.965554 ==7.589307 ==5.9129953 ==8.45648 ==29.533484 ==28.773294 ==21.873348 ==29.172363 ==11.006105 ==27.684134 ==23.428614 ===\n",
      "Loss in epoch:7 = 21.457211077213287\n",
      "Dev Loss in epoch:7 = 33.032676696777344\n",
      "Epoch: 8\n",
      "Starting..  18.27856 ==13.003601 ==35.806194 ==68.42859 ==44.675785 ==30.92334 ==21.158148 ==8.678507 ==10.359045 ==20.322462 ==25.08356 ==8.467877 ==32.574867 ==26.893744 ==37.552113 ==27.220913 ===\n",
      "Loss in epoch:8 = 26.839206635951996\n",
      "Dev Loss in epoch:8 = 21.59126853942871\n",
      "SAVING..\n",
      "Epoch: 9\n",
      "Starting..  5.869227 ==12.268181 ==22.472342 ==33.734367 ==34.085217 ==10.99272 ==13.424227 ==5.103622 ==7.8658934 ==11.099295 ==13.2478695 ==7.268621 ==23.852762 ==15.624899 ==28.734531 ==20.947277 ===\n",
      "Loss in epoch:9 = 16.66194060444832\n",
      "Dev Loss in epoch:9 = 25.486248016357422\n",
      "Dev Loss Currently :  21.591269\n",
      "Epoch: 0\n",
      "Starting..  5.869227 ==12.268181 ==22.472342 ==33.734367 ==34.085217 ==10.99272 ==13.424227 ==5.103622 ==7.8658934 ==11.099295 ==13.2478695 ==7.268621 ==23.852762 ==15.624899 ==28.734531 ==20.947277 ===\n",
      "Loss in epoch:0 = 16.66194060444832\n",
      "Dev Loss in epoch:0 = 25.486248016357422\n",
      "Epoch: 1\n",
      "Starting..  5.9904313 ==10.226198 ==27.76479 ==46.227028 ==35.15204 ==13.751775 ==11.938068 ==5.6508093 ==8.897926 ==9.882065 ==11.233229 ==12.334045 ==28.641672 ==15.180598 ==29.102154 ==20.944801 ===\n",
      "Loss in epoch:1 = 18.307351797819138\n",
      "Dev Loss in epoch:1 = 23.66788101196289\n",
      "Epoch: 2\n",
      "Starting..  3.2379677 ==11.902411 ==24.81863 ==39.871162 ==34.853558 ==13.194919 ==8.996909 ==4.630809 ==7.6440363 ==12.5548525 ==14.394748 ==6.340573 ==21.168392 ==16.343502 ==30.735363 ==21.823895 ===\n",
      "Loss in epoch:2 = 17.031982943415642\n",
      "Dev Loss in epoch:2 = 24.50166893005371\n",
      "Epoch: 3\n",
      "Starting..  4.7927556 ==7.660727 ==20.752666 ==35.533928 ==28.074574 ==10.276817 =="
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "#     tf.reset_default_graph()\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    prev_dev_loss = 100\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "      try:\n",
    "        saver.restore(sess,path)\n",
    "        dev_loss = sess.run(loss,feed_dict={input_bow:dev_seq_bow,input_pose:dev_seq_pose,input_gaze:dev_seq_gaze, input_au:dev_seq_au,\n",
    "                X_asr:val_data_asr,\n",
    "                true_targets: dev_target})\n",
    "        print('Dev Loss Currently : ',dev_loss)\n",
    "        prev_dev_loss = dev_loss\n",
    "      except:\n",
    "        print('No pre-saved weights found')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "      for ep in range(EPOCHS):\n",
    "\n",
    "        num_bts = 0\n",
    "        ep_loss = 0\n",
    "        tr_losses = []\n",
    "        print(\"Epoch:\",ep)\n",
    "        print('Starting.. ', end = ' ')\n",
    "        for j in range(len(target)//BATCH_SIZE):\n",
    "    #       tr_feat = train_seq_feat[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          tr_pose = train_seq_pose[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          tr_gaze = train_seq_gaze[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          tr_au = train_seq_au[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          tr_bovw = train_seq_bow[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "    #       tr_audio_mfcc = train_data_mfcc[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "    #       tr_audio_ege=train_data_ege[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "    #       tr_BOAW_mfcc=train_data_BOAW_mfcc[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "    #       tr_BOAW_ege=train_data_BOAW_ege[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          tr_asr=train_data_asr[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "\n",
    "          lb = target[j*BATCH_SIZE:j*BATCH_SIZE+BATCH_SIZE]\n",
    "          _,l = sess.run([optimizer,loss],feed_dict={input_bow:tr_bovw,input_pose:tr_pose, input_gaze:tr_gaze, input_au:tr_au,\n",
    "            X_asr:tr_asr,\n",
    "            true_targets: lb}) # THIS LINE!!!\n",
    "          print(l,'==',end = '')\n",
    "          num_bts+=1\n",
    "          ep_loss+=l\n",
    "        print(\"=\")\n",
    "        ep_loss = ep_loss/num_bts\n",
    "        print(\"Loss in epoch:{} = {}\".format(ep,ep_loss))\n",
    "        dev_loss = sess.run(loss,feed_dict={input_bow:dev_seq_bow,input_pose:dev_seq_pose,input_gaze:dev_seq_gaze, input_au:dev_seq_au,\n",
    "                X_asr:val_data_asr,\n",
    "                true_targets: dev_target})\n",
    "        print(\"Dev Loss in epoch:{} = {}\".format(ep,dev_loss))\n",
    "        tr_losses.append(ep_loss)\n",
    "        if dev_loss<prev_dev_loss:\n",
    "          print('SAVING..')\n",
    "          save_path = saver.save(sess, path)\n",
    "          prev_dev_loss = dev_loss\n",
    "\n",
    "        tr_losses.append(ep_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
